from openai import OpenAI
import streamlit as st
from langchain.chat_models import ChatOpenAI
from langchain.chains.question_answering import load_qa_chain
from langchain.chains import RetrievalQAWithSourcesChain
from langchain.text_splitter import CharacterTextSplitter
from langchain.document_loaders import UnstructuredURLLoader
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from inject import DocLoad

st.title("ChatGPT ðŸ¤–")

llm = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])


################################################################
# Development of Chaining
################################################################
# Step 1: Create a File Upload object
uploaded_file = st.file_uploader('Choose your .pdf file', type="pdf")

# Step 2: Read the file and convert it into documentation object
docs = DocLoad(file_path=uploaded_file)

# Step 3: Document object is split into chunks of text
chunks = docs.split_text()

# Step 4: Chunk Text are converted into with embedding of OpenAI into VectorDB
vectordb = VectorStore(embeddings=OpenAIEmbeddings(), docs=chunks)
retriever = vectordb.as_retriever()

# Step 5: Generate the prompt for the chatbot
template = '''Answer the question based on the context below. If the
question cannot be answered using the information provided answer
with "I don't know".
Question: {question}
**Answer**:/n{answer} 
'''
prompt = PromptTemplate(template=template, input_variables=[
                        "question", "answer"])

# Step 6: Define the LLM Model object of OPENAI
llm = ChatOpenAI(model_name="gpt-4")

# Step 7: Define the Load Question answer chain with source chain
qa_chain = load_qa_chain(llm, chain_type="refine",  prompt=prompt)

# Step 8: Define the vectordb
chain = RetrievalQAWithSourcesChain(combine_documents_chain=qa_chain,
                                    retriever=retriever,
                                    return_source_documents=True)


################################################################
# Development of Chatbot API
################################################################

if "openai_model" not in st.session_state:
    st.session_state["openai_model"] = "gpt-4"

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])


def generate_response(prompt_input):
    """
    The function `generate_response` takes a prompt input and returns the output generated by a chain
    model.

    :param prompt_input: The input provided by the user as a prompt or question
    :return: The output of the function is being returned.
    """
    output = chain({"question": prompt_input}, return_only_outputs=True)
    return output


# The code `if prompt := st.chat_input("What is up?"):` is checking if the user has entered any input
# in the chat input box. If there is an input, it assigns the input value to the variable `prompt`.
if prompt := st.chat_input("What is up?"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            print("*" * 25)
            print("\nUser's Question:\n", prompt)
            # The line `response = generate_response(prompt)` is calling the `generate_response`
            # function and passing the user's input prompt as an argument. This function uses the
            # defined chain model to generate a response based on the input prompt. The generated
            # response is then stored in the `response` variable for further use.
            response = generate_response(prompt)

            # `message_placeholder = st.empty()` is creating an empty placeholder in the Streamlit
            # app. This placeholder will be used to display the response generated by the chatbot.
            message_placeholder = st.empty()

            # `full_response = ""` is initializing an empty string variable called `full_response`.
            # This variable will be used to store the complete response generated by the chatbot.
            full_response = ""

            for response in llm.chat.completions.create(model=st.session_state["openai_model"],
                                                        messages=[{"role": m["role"], "content": m["content"]}
                                                                  for m in st.session_state.messages],
                                                        stream=True,):
                full_response += (response.choices[0].delta.content or "")
                message_placeholder.markdown(full_response + "â–Œ")
            message_placeholder.markdown(full_response)
        st.session_state.messages.append(
            {"role": "assistant", "content": full_response})
